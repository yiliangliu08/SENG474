{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.2"},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{},"toc_section_display":true,"toc_window_display":true},"varInspector":{"cols":{"lenName":16,"lenType":16,"lenVar":40},"kernels_config":{"python":{"delete_cmd_postfix":"","delete_cmd_prefix":"del ","library":"var_list.py","varRefreshCmd":"print(var_dic_list())"},"r":{"delete_cmd_postfix":") ","delete_cmd_prefix":"rm(","library":"var_list.r","varRefreshCmd":"cat(var_dic_list()) "}},"types_to_exclude":["module","function","builtin_function_or_method","instance","_Feature"],"window_display":false},"colab":{"name":"SENG 474 Project.ipynb","provenance":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"x9q4qfd23ox3"},"source":["This report is written by Ye Yuan V00886654, Yiliang Liu V00869672, Weiyi Zhang V00868237"]},{"cell_type":"markdown","metadata":{"ExecuteTime":{"end_time":"2019-02-26T22:50:12.194390Z","start_time":"2019-02-26T22:50:12.186390Z"},"id":"TqpwD_Tz3oyD"},"source":["## Overview\n","> 1. Download the Communities and Crime data1\n","from https://archive.ics.uci.edu/ml/datasets/Communities+and+Crime. Use the frst 1495 rows of data as the training set and the rest as the test set.  \n","2. The data set has missing values. Use a data imputation technique to deal with the missing values in the data set. The data description mentions some features are nonpredictive. Ignore those features.  \n","3. Plot a correlation matrix for the features in the data set.   \n","4.  Calculate the Coefficient of Variation CV for each feature, where CV = s/m, in which s is sample variance and m is sample mean..  \n","5.  Pick 128 features with highest CV , and make scatter plots and box plots for them.\n","6.  Fit a linear model using least squares to the training set and report the test error.    \n","7.  Fit a ridge regression model on the training set, with \\$\\lambda\\$  chosen by cross-validation. Report the test error obtained.  \n","8.  Fit a LASSO model on the training set, with \\$\\lambda\\$ chosen by cross-validation. Report the test error obtained, along with a list of the variables selected by the model. Repeat with standardized features. Report the test error for both cases and compare them.\n","9.  Fit a PCR model on the training set, with M (the number of principal components) chosen by cross-validation. Report the test error obtained.  \n","10. In this section, we would like to fit a boosting tree to the data. As in classification trees, one can use any type of regression at each node to build a multivariate regression tree. Because the number of variables is large in this problem, one can use L1-penalized regression at each node. Such a tree is called L1 penalized gradient boosting tree.\n","\n","*Note: If you want to rerun the code, it may cause error because lack of model.(We encountered this error so we want to mention it here.)"]},{"cell_type":"markdown","metadata":{"id":"DzejL_Sh3oyF"},"source":["## Data Cleaning and Data Preparation"]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2019-11-26T14:30:07.498019Z","start_time":"2019-11-26T14:30:07.485708Z"},"id":"9PVnxIim3oyI","executionInfo":{"status":"ok","timestamp":1615140425818,"user_tz":480,"elapsed":1818,"user":{"displayName":"Yiliang Liu","photoUrl":"","userId":"13009448635366228041"}}},"source":["# import  libaries\n","import pandas as pd \n","import numpy as np \n","import matplotlib.pyplot as plt \n","from sklearn.metrics import make_scorer, r2_score, mean_squared_error, auc, mean_absolute_error\n","from sklearn.model_selection import GridSearchCV, KFold\n","from matplotlib import style\n","style.use(\"ggplot\")\n","import seaborn as sns\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","\n","%matplotlib inline"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2019-11-26T14:30:07.978037Z","start_time":"2019-11-26T14:30:07.826693Z"},"scrolled":true,"id":"-P9vk-G93oyL","executionInfo":{"status":"error","timestamp":1615140425874,"user_tz":480,"elapsed":1846,"user":{"displayName":"Yiliang Liu","photoUrl":"","userId":"13009448635366228041"}},"outputId":"8123509d-05cb-4482-fa05-ab8b80c974cf","colab":{"base_uri":"https://localhost:8080/","height":472}},"source":["# read data\n","df = pd.read_csv('communities.txt',header=None,names=[\"col_\" + str(i) for i in range(127)] + ['goal'])\n","# set train size \n","train_size = 1495\n","df"],"execution_count":2,"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-34cec1895aaf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# read data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'communities.txt'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"col_\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m127\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'goal'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m# set train size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtrain_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1495\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    686\u001b[0m     )\n\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 688\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    946\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 948\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1180\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1181\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   2008\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2009\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2010\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2011\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2012\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n","\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'communities.txt'"]}]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2019-11-26T14:30:08.351438Z","start_time":"2019-11-26T14:30:08.088468Z"},"scrolled":true,"id":"Jyi8dobO3oyO","executionInfo":{"status":"aborted","timestamp":1615140425837,"user_tz":480,"elapsed":1797,"user":{"displayName":"Yiliang Liu","photoUrl":"","userId":"13009448635366228041"}}},"source":["# print df base info\n","df.describe()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZZTECWPm3oyQ"},"source":["### Pre-process"]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2019-11-26T14:30:08.729837Z","start_time":"2019-11-26T14:30:08.458534Z"},"scrolled":true,"id":"uEGPDz8T3oyQ","executionInfo":{"status":"aborted","timestamp":1615140425839,"user_tz":480,"elapsed":1787,"user":{"displayName":"Yiliang Liu","photoUrl":"","userId":"13009448635366228041"}}},"source":["# Convert ? to NaN\n","# define function\n","def getNaN(x):\n","    if str(x) == '?':return np.nan\n","    else :return x \n","    \n","# Convert ? to NaN\n","for i in df.columns:\n","    df[i] = df[i].apply(getNaN)\n","df"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2019-11-26T14:30:08.880979Z","start_time":"2019-11-26T14:30:08.847044Z"},"scrolled":true,"id":"pvKVXcv73oyS","executionInfo":{"status":"aborted","timestamp":1615140425841,"user_tz":480,"elapsed":1779,"user":{"displayName":"Yiliang Liu","photoUrl":"","userId":"13009448635366228041"}}},"source":["df_null = df.isnull().sum() / df.shape[0]\n","df_null = df_null.reset_index().rename(columns = {\"index\":\"columns\",0:\"ration\"})\n","df_null['rate_num'] = list(df.isnull().sum())\n","df_null = df_null.sort_values(by=['ration'],ascending=False)\n","df_null"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"ExecuteTime":{"end_time":"2019-11-26T14:17:11.286267Z","start_time":"2019-11-26T14:17:11.283206Z"},"id":"1ISwtaC63oyT"},"source":["### Data missing ratio"]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2019-11-26T14:30:09.095456Z","start_time":"2019-11-26T14:30:09.031202Z"},"scrolled":true,"id":"s9LfOTlO3oyU","executionInfo":{"status":"aborted","timestamp":1615140425843,"user_tz":480,"elapsed":1769,"user":{"displayName":"Yiliang Liu","photoUrl":"","userId":"13009448635366228041"}}},"source":["# rebuild data set \n","# ignore nonpredictive features\n","df1 = df.iloc[:,5:].copy()\n","df1"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XMu-xla43oyX"},"source":["### Split data and clean Nan"]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2019-11-26T14:30:09.511226Z","start_time":"2019-11-26T14:30:09.388001Z"},"scrolled":true,"id":"cTtnEf8R3oyY","executionInfo":{"status":"aborted","timestamp":1615140425844,"user_tz":480,"elapsed":1758,"user":{"displayName":"Yiliang Liu","photoUrl":"","userId":"13009448635366228041"}}},"source":["# fill missing values\n","df_obj = df1.select_dtypes(\"object\").copy()\n","# use median to fill missing values\n","df_obj = df_obj.fillna(df_obj.median())\n","df_obj"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2019-11-26T14:30:09.979805Z","start_time":"2019-11-26T14:30:09.891709Z"},"scrolled":true,"id":"IHFfX9Ol3oya","executionInfo":{"status":"aborted","timestamp":1615140425846,"user_tz":480,"elapsed":1756,"user":{"displayName":"Yiliang Liu","photoUrl":"","userId":"13009448635366228041"}}},"source":["# select float features and use mean to fill missing data\n","df_float = df1.select_dtypes(\"float64\").copy()\n","df_float = df_float.fillna(df_float.mean())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2019-11-26T14:30:10.533115Z","start_time":"2019-11-26T14:30:10.438901Z"},"scrolled":true,"id":"wQXFkvyZ3oya","executionInfo":{"status":"aborted","timestamp":1615140425848,"user_tz":480,"elapsed":1746,"user":{"displayName":"Yiliang Liu","photoUrl":"","userId":"13009448635366228041"}}},"source":["# concat obj features and float features\n","df1 = pd.concat([df_obj,df_float],axis=1)\n","df1"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2019-11-26T14:30:11.866087Z","start_time":"2019-11-26T14:30:10.676592Z"},"id":"jnRDHo_C3oyb","executionInfo":{"status":"aborted","timestamp":1615140425849,"user_tz":480,"elapsed":1736,"user":{"displayName":"Yiliang Liu","photoUrl":"","userId":"13009448635366228041"}}},"source":["# get Correlation coefficient about all features\n","corr = df1.corr()\n","# plot Correlation coefficient\n","plt.figure(figsize=(16,12))\n","sns.heatmap(corr)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2019-11-26T14:30:12.094181Z","start_time":"2019-11-26T14:30:11.867818Z"},"scrolled":true,"id":"VEni6ol13oyc","executionInfo":{"status":"aborted","timestamp":1615140425853,"user_tz":480,"elapsed":1732,"user":{"displayName":"Yiliang Liu","photoUrl":"","userId":"13009448635366228041"}}},"source":["# get CV s / m\n","coe_var = df1.std() / df1.mean()\n","coe_var = coe_var.reset_index().rename(columns = {\"index\":\"columns\",0:\"cv\"}).sort_values(\"cv\",ascending = False)\n","coe_var"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2019-11-26T14:30:12.358187Z","start_time":"2019-11-26T14:30:12.354016Z"},"id":"UCi1b-pe3oyc","executionInfo":{"status":"aborted","timestamp":1615140425855,"user_tz":480,"elapsed":1722,"user":{"displayName":"Yiliang Liu","photoUrl":"","userId":"13009448635366228041"}}},"source":["high_cv = coe_var.iloc[:int(np.sqrt(df1.shape[1] - 1)),0].values\n","high_cv"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vseNb40o3oyd"},"source":["## Exploratory Data Analysis"]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2019-11-26T14:30:13.137219Z","start_time":"2019-11-26T14:30:12.700576Z"},"id":"75wx7_rD3oye","executionInfo":{"status":"aborted","timestamp":1615140425857,"user_tz":480,"elapsed":1716,"user":{"displayName":"Yiliang Liu","photoUrl":"","userId":"13009448635366228041"}}},"source":["temp = df1[high_cv]\n","# box plot height cv of features \n","plt.figure(figsize=(12,8))\n","temp.boxplot()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2019-11-26T14:30:27.262592Z","start_time":"2019-11-26T14:30:13.281732Z"},"scrolled":true,"id":"dju6yCM83oyf","executionInfo":{"status":"aborted","timestamp":1615140425859,"user_tz":480,"elapsed":1708,"user":{"displayName":"Yiliang Liu","photoUrl":"","userId":"13009448635366228041"}}},"source":["# scatter height cv of features \n","sns.pairplot(temp)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xrRYiWXe3oyf"},"source":["## Data Mining"]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2019-11-26T14:34:12.428615Z","start_time":"2019-11-26T14:34:12.418872Z"},"id":"hq9IplN_3oyg","executionInfo":{"status":"aborted","timestamp":1615140425861,"user_tz":480,"elapsed":1708,"user":{"displayName":"Yiliang Liu","photoUrl":"","userId":"13009448635366228041"}}},"source":["# split train and test \n","train = df1.iloc[:train_size,:]\n","test = df1.iloc[train_size:,:]\n","train_y = train.pop('goal')\n","test_y = test.pop(\"goal\")\n","train_X = train\n","test_X = test"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1kV05cId3oyg"},"source":["### 1. Linear Regression"]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2019-11-26T14:34:13.683758Z","start_time":"2019-11-26T14:34:13.595465Z"},"scrolled":true,"id":"35ZWBb5b3oyg","executionInfo":{"status":"aborted","timestamp":1615140425863,"user_tz":480,"elapsed":1701,"user":{"displayName":"Yiliang Liu","photoUrl":"","userId":"13009448635366228041"}}},"source":["# import libraries\n","from sklearn.linear_model import LinearRegression\n","from sklearn.metrics import mean_squared_error,r2_score\n","\n","# Fit a linear model using least squares to the training set and report the test error.   \n","lr = LinearRegression(normalize=True)\n","lr.fit(train_X,train_y)\n","y_pre = lr.predict(test_X)\n","# process  abnormal data\n","y_train = lr.predict(train_X)\n","y_pre = [0 if i < min(y_train) or i > max(y_train) else i for i in y_pre]\n","# print train and test error\n","print(\"train mse is \",mean_squared_error(train_y,y_train))\n","print(\"test mse is \",mean_squared_error(test_y,y_pre))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2019-11-26T14:34:14.273326Z","start_time":"2019-11-26T14:34:14.179204Z"},"id":"x7YQ3Qcz3oyi","executionInfo":{"status":"aborted","timestamp":1615140425864,"user_tz":480,"elapsed":1695,"user":{"displayName":"Yiliang Liu","photoUrl":"","userId":"13009448635366228041"}}},"source":["# use pca model\n","from sklearn.linear_model import LinearRegression\n","from sklearn.metrics import mean_squared_error\n","# from sklearn.cross_validation import cross_val_score\n","from sklearn.model_selection import cross_val_predict,cross_val_score,cross_validate\n","from sklearn.decomposition import PCA\n","\n","temp = pd.concat([train_X,test_X])\n","size = train_X.shape[0]\n","\n","# use pca\n","pca = PCA(n_components=0.95)\n","pca.fit(temp)\n","tt = pca.transform(temp)\n","\n","train_X = tt[:size,:]\n","test_X = tt[size:,:]\n","\n","# Fit a linear model using least squares to the training set and report the test error.   \n","lr = LinearRegression(normalize=True)\n","lr.fit(train_X,train_y)\n","y_pre = lr.predict(test_X)\n","# process  abnormal data\n","y_train = lr.predict(train_X)\n","# print train and test error\n","print(\"train mse is \",mean_squared_error(train_y,y_train))\n","print(\"test mse is \",mean_squared_error(test_y,y_pre))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MZhMUy2i3oyi"},"source":["###  2. Ridge Regularization"]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2019-11-26T14:34:15.855572Z","start_time":"2019-11-26T14:34:14.543988Z"},"scrolled":false,"id":"F5Pd0mI33oyi","executionInfo":{"status":"aborted","timestamp":1615140425869,"user_tz":480,"elapsed":1693,"user":{"displayName":"Yiliang Liu","photoUrl":"","userId":"13009448635366228041"}}},"source":["from sklearn.linear_model import Ridge\n","from sklearn.metrics import mean_squared_error\n","from sklearn.model_selection import cross_val_score\n","\n","\n","# search for an optimal value of alpha for ridge model\n","alphas = np.linspace(0.1,10,50)\n","for k in alphas:\n","    rd = Ridge(alpha=k)\n","    # 10 fold cross-validation\n","    scores = cross_val_score(rd, train_X, train_y, cv=10, scoring='neg_mean_squared_error')\n","    # print train error\n","    print(\"alpha is \",k,\"train mse is \",-scores.mean())\n","# Fit a ridge regression model on the training set, with λ chosen by cross-validation. Report the test error obtained.\n","ridge = Ridge(alpha=2.726)\n","ridge.fit(train_X,train_y)\n","y_pre = ridge.predict(test_X)\n","# print test error\n","print(\"test mse is \",mean_squared_error(test_y,y_pre))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"ExecuteTime":{"end_time":"2019-11-26T14:36:05.591048Z","start_time":"2019-11-26T14:36:05.587729Z"},"id":"bNesEtwx3oyj"},"source":["### 3. Lasso Regularization"]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2019-11-26T14:34:17.120440Z","start_time":"2019-11-26T14:34:16.115685Z"},"scrolled":false,"id":"3KDPvBtw3oyk","executionInfo":{"status":"aborted","timestamp":1615140425871,"user_tz":480,"elapsed":1688,"user":{"displayName":"Yiliang Liu","photoUrl":"","userId":"13009448635366228041"}}},"source":["from sklearn.linear_model import Lasso\n","from sklearn.metrics import mean_squared_error\n","# from sklearn.cross_validation import cross_val_score\n","from sklearn.model_selection import cross_val_predict,cross_val_score,cross_validate\n","\n","\n","# search for an optimal value of alpha for Lasso model\n","alphas = np.linspace(0.1,10,50)\n","for k in alphas:\n","    rd = Lasso(alpha=k)\n","    scores = cross_val_score(rd, train_X, train_y, cv=10, scoring='neg_mean_squared_error')\n","    print(\"alpha is \",k,\"train mse is \",-scores.mean())\n","    \n","# Fit a LASSO model on the training set, with λ chosen by cross-validation. Report the test error obtained, \n","# along with a list of the variables selected by the model. \n","lass = Lasso(alpha = 0.5)\n","lass.fit(train_X,train_y)\n","y_pre = lass.predict(test_X)\n","print(\"test mse is \",mean_squared_error(test_y,y_pre))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"ExecuteTime":{"end_time":"2019-11-26T14:36:34.213673Z","start_time":"2019-11-26T14:36:34.211177Z"},"id":"xeD1SHBk3oyk"},"source":["### 4. StandardScaler and Lasso"]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2019-11-26T14:34:20.112859Z","start_time":"2019-11-26T14:34:17.378144Z"},"scrolled":false,"id":"ULtGDLTh3oyk","executionInfo":{"status":"aborted","timestamp":1615140425872,"user_tz":480,"elapsed":1678,"user":{"displayName":"Yiliang Liu","photoUrl":"","userId":"13009448635366228041"}}},"source":["# import libraries\n","from sklearn.linear_model import Lasso\n","from sklearn.metrics import mean_squared_error\n","from sklearn.model_selection import cross_val_score\n","from sklearn.preprocessing import StandardScaler\n","\n","# standar all train_X and test_X data\n","# Repeat with standardized features. Report the test error for both cases and compare them.\n","std = StandardScaler()\n","ss_train = std.fit_transform(train)\n","ss_test = std.transform(test)\n","\n","# search for an optimal value of alpha for Lasso model\n","alphas = np.linspace(0.1,5,50)\n","for k in alphas:\n","    rd = Lasso(alpha=k)\n","    scores = cross_val_score(rd, ss_train, train_y, cv=10, scoring='neg_mean_squared_error')\n","    print(\"alpha is \",k,\"train mse is \",-scores.mean())\n","    \n","lass = Lasso(alpha=0.1)\n","lass.fit(ss_train,train_y)\n","y_pre = lass.predict(ss_test)\n","\n","print(\"test mse is \",mean_squared_error(test_y,y_pre))\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"meuN5gmG3oyl"},"source":["### Conclusion"]},{"cell_type":"markdown","metadata":{"id":"fLovyuEX3oyl"},"source":["We first use linear regression to test the data, and get the result with mse = 0.016643431264981627 for train set and 0.0218767918707957 for test set. Then we fit the data set with PCA model. Then we test the data using linear regression, ridge Regularization,  Lasso Regularization and  StandardScaler and Lasso. We got the mse of the predict as follows."]},{"cell_type":"markdown","metadata":{"id":"tvX1udmB3oyl"},"source":["![predictions.png](attachment:predictions.png)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"UeNNRDI03oym"},"source":["As we can see, linear regression and ridge Regularization have the smaller MSEs, so for this data set,in these four algorithms, linear regression and ridge Regularization is a better choice."]},{"cell_type":"code","metadata":{"id":"CMn6lhEI3oyn","executionInfo":{"status":"aborted","timestamp":1615140425873,"user_tz":480,"elapsed":1676,"user":{"displayName":"Yiliang Liu","photoUrl":"","userId":"13009448635366228041"}}},"source":[""],"execution_count":null,"outputs":[]}]}